{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "RIFuVyYAJ85E",
    "outputId": "3d6b06ad-e0cc-4a6a-a6f2-aab7a8f52856"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 0/200000, time : 1453, reward:-19.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Dumping : weights_0.hdf5\n",
      "Episode : 1/200000, time : 1422, reward:-18.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Episode : 2/200000, time : 1304, reward:-19.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Episode : 3/200000, time : 1229, reward:-19.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Episode : 4/200000, time : 1052, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Episode : 5/200000, time : 1394, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Episode : 6/200000, time : 1256, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Episode : 7/200000, time : 1320, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Episode : 8/200000, time : 1273, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Episode : 9/200000, time : 1224, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Episode : 10/200000, time : 993, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Dumping : weights_10.hdf5\n",
      "games : [10]\n",
      "avg_reward : [-19]\n",
      "Episode : 11/200000, time : 1266, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Episode : 12/200000, time : 1160, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Episode : 13/200000, time : 1460, reward:-19.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Episode : 14/200000, time : 1319, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Episode : 15/200000, time : 1296, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Episode : 16/200000, time : 1211, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Episode : 17/200000, time : 1519, reward:-18.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Episode : 18/200000, time : 1170, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Episode : 19/200000, time : 1261, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Episode : 20/200000, time : 1098, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Dumping : weights_20.hdf5\n",
      "games : [10, 20]\n",
      "avg_reward : [-19, -18]\n",
      "Episode : 21/200000, time : 1201, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Episode : 22/200000, time : 1057, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Episode : 23/200000, time : 1379, reward:-19.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Episode : 24/200000, time : 1054, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Episode : 25/200000, time : 1290, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Episode : 26/200000, time : 1093, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Episode : 27/200000, time : 1335, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Episode : 28/200000, time : 1385, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Episode : 29/200000, time : 1136, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Episode : 30/200000, time : 1308, reward:-19.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Dumping : weights_30.hdf5\n",
      "games : [10, 20, 30]\n",
      "avg_reward : [-19, -18, -18]\n",
      "Episode : 31/200000, time : 1099, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Episode : 32/200000, time : 1106, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Episode : 33/200000, time : 1127, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Episode : 34/200000, time : 1389, reward:-19.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Episode : 35/200000, time : 1448, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Episode : 36/200000, time : 1240, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Episode : 37/200000, time : 1047, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Episode : 38/200000, time : 1023, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Episode : 39/200000, time : 1395, reward:-19.0, done:True, info:{'ale.lives': 0}, Epsilon : 1.0\n",
      "Episode : 40/200000, time : 1158, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.9877256433042569\n",
      "Dumping : weights_40.hdf5\n",
      "games : [10, 20, 30, 40]\n",
      "avg_reward : [-19, -18, -18, -18]\n",
      "Episode : 41/200000, time : 1095, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.9742857395472233\n",
      "Episode : 42/200000, time : 1190, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.9598761398976476\n",
      "Episode : 43/200000, time : 1097, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.946767841344899\n",
      "Episode : 44/200000, time : 1104, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.9337918609302915\n",
      "Episode : 45/200000, time : 1392, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.9176840244371524\n",
      "Episode : 46/200000, time : 1109, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.9050161455241608\n",
      "Episode : 47/200000, time : 1203, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.8914972990561768\n",
      "Updating target_model weights\n",
      "Done updating target model weights\n",
      "Episode : 48/200000, time : 1153, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.8787514099551227\n",
      "Episode : 49/200000, time : 1127, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.8664476528130334\n",
      "Episode : 50/200000, time : 1272, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.8527797412840386\n",
      "Dumping : weights_50.hdf5\n",
      "games : [10, 20, 30, 40, 50]\n",
      "avg_reward : [-19, -18, -18, -18, -18]\n",
      "Episode : 51/200000, time : 1148, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.8405874037171845\n",
      "Episode : 52/200000, time : 1538, reward:-19.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.8246016799581717\n",
      "Episode : 53/200000, time : 1089, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.8134220620147601\n",
      "Episode : 54/200000, time : 1203, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.8012714191511678\n",
      "Episode : 55/200000, time : 1135, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.7899734877088385\n",
      "Episode : 56/200000, time : 1123, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.7789516940744132\n",
      "Updating target_model weights\n",
      "Done updating target model weights\n",
      "Episode : 57/200000, time : 1092, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.7683909802803716\n",
      "Episode : 58/200000, time : 1032, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.7585421519364759\n",
      "Episode : 59/200000, time : 1107, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.7481085021015463\n",
      "Episode : 60/200000, time : 1288, reward:-19.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.7361232912439495\n",
      "Dumping : weights_60.hdf5\n",
      "games : [10, 20, 30, 40, 50, 60]\n",
      "avg_reward : [-19, -18, -18, -18, -18, -18]\n",
      "Episode : 61/200000, time : 1089, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.7261795366878424\n",
      "Episode : 62/200000, time : 1103, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.7162268418094816\n",
      "Episode : 63/200000, time : 1125, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.7061986574783526\n",
      "Episode : 64/200000, time : 1046, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.6970424105485886\n",
      "Episode : 65/200000, time : 1051, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.687936080536572\n",
      "Updating target_model weights\n",
      "Done updating target model weights\n",
      "Episode : 66/200000, time : 1264, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.6771179831113122\n",
      "Episode : 67/200000, time : 1238, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.6667366530202425\n",
      "Episode : 68/200000, time : 1094, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.657664420757545\n",
      "Episode : 69/200000, time : 1120, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.6485210432681018\n",
      "Episode : 70/200000, time : 1302, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.6380355768371399\n",
      "Dumping : weights_70.hdf5\n",
      "games : [10, 20, 30, 40, 50, 60, 70]\n",
      "avg_reward : [-19, -18, -18, -18, -18, -18, -18]\n",
      "Episode : 71/200000, time : 1117, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.6291650952562802\n",
      "Episode : 72/200000, time : 1112, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.6204799846618394\n",
      "Episode : 73/200000, time : 1136, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.6117312134458724\n",
      "Updating target_model weights\n",
      "Done updating target model weights\n",
      "Episode : 74/200000, time : 1119, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.6032264361915889\n",
      "Episode : 75/200000, time : 1106, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.5949291340702406\n",
      "Episode : 76/200000, time : 1122, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.5866286198822925\n",
      "Episode : 77/200000, time : 1089, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.5787042801947925\n",
      "Episode : 78/200000, time : 1102, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.5707728158191238\n",
      "Episode : 79/200000, time : 1049, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.5633442694322762\n",
      "Episode : 80/200000, time : 1052, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.5559568049217426\n",
      "Dumping : weights_80.hdf5\n",
      "games : [10, 20, 30, 40, 50, 60, 70, 80]\n",
      "avg_reward : [-19, -18, -18, -18, -18, -18, -18, -18]\n",
      "Episode : 81/200000, time : 1127, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.5481726267892737\n",
      "Episode : 82/200000, time : 1093, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.5407407227761134\n",
      "Updating target_model weights\n",
      "Done updating target model weights\n",
      "Episode : 83/200000, time : 1106, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.5333029036140161\n",
      "Episode : 84/200000, time : 1122, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.5258622051099168\n",
      "Episode : 85/200000, time : 1118, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.5185771764393459\n",
      "Episode : 86/200000, time : 1040, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.5118535433237386\n",
      "Episode : 87/200000, time : 1045, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.5052170857421256\n",
      "Episode : 88/200000, time : 1112, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.4982429762485211\n",
      "Episode : 89/200000, time : 1110, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.49136513864397474\n",
      "Episode : 90/200000, time : 1134, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.4844368874710691\n",
      "Dumping : weights_90.hdf5\n",
      "games : [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
      "avg_reward : [-19, -18, -18, -18, -18, -18, -18, -18, -18]\n",
      "Episode : 91/200000, time : 1124, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.4776779727914194\n",
      "Updating target_model weights\n",
      "Done updating target model weights\n",
      "Episode : 92/200000, time : 1098, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.47115468794334636\n",
      "Episode : 93/200000, time : 1124, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.46458108791600267\n",
      "Episode : 94/200000, time : 1132, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.45805339467625344\n",
      "Episode : 95/200000, time : 1117, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.4516851695364148\n",
      "Episode : 96/200000, time : 1138, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.4452941403271112\n",
      "Episode : 97/200000, time : 1091, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.4392570214583387\n",
      "Episode : 98/200000, time : 1192, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.4327604490817161\n",
      "Episode : 99/200000, time : 1182, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.42640259962812566\n",
      "Episode : 100/200000, time : 1064, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.42076885168002565\n",
      "Dumping : weights_100.hdf5\n",
      "games : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
      "avg_reward : [-19, -18, -18, -18, -18, -18, -18, -18, -18, -18]\n",
      "Updating target_model weights\n",
      "Done updating target model weights\n",
      "Episode : 101/200000, time : 1095, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.4150434835994536\n",
      "Episode : 102/200000, time : 1032, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.4097236761965454\n",
      "Episode : 103/200000, time : 1037, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.40443160907112397\n",
      "Episode : 104/200000, time : 1040, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.39920789526792183\n",
      "Episode : 105/200000, time : 1058, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.3939531488605482\n",
      "Episode : 106/200000, time : 1116, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.38849552145089206\n",
      "Episode : 107/200000, time : 1117, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.383113501247394\n",
      "Episode : 108/200000, time : 1062, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.3780517007250139\n",
      "Episode : 109/200000, time : 1123, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.3727770834202997\n",
      "Updating target_model weights\n",
      "Done updating target model weights\n",
      "Episode : 110/200000, time : 1131, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.3675393014470999\n",
      "Dumping : weights_110.hdf5\n",
      "games : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110]\n",
      "avg_reward : [-19, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18]\n",
      "Episode : 111/200000, time : 1162, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.36223018933743717\n",
      "Episode : 112/200000, time : 1100, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.35728348708149826\n",
      "Episode : 113/200000, time : 1017, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.35275692759510735\n",
      "Episode : 114/200000, time : 1122, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.34785261827075203\n",
      "Episode : 115/200000, time : 1228, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.3425365935029492\n",
      "Episode : 116/200000, time : 1121, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.3377743754484315\n",
      "Episode : 117/200000, time : 1046, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.3333782785307704\n",
      "Episode : 118/200000, time : 1053, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.3290229443686069\n",
      "Updating target_model weights\n",
      "Done updating target model weights\n",
      "Episode : 119/200000, time : 1131, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.3243999390299446\n",
      "Episode : 120/200000, time : 1044, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.32017790895390447\n",
      "Dumping : weights_120.hdf5\n",
      "games : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120]\n",
      "avg_reward : [-19, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18]\n",
      "Episode : 121/200000, time : 1134, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.31567918262569605\n",
      "Episode : 122/200000, time : 1103, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.3113526236614296\n",
      "Episode : 123/200000, time : 1105, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.3070700083286146\n",
      "Episode : 124/200000, time : 1165, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.302634376293312\n",
      "Episode : 125/200000, time : 1120, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.2984119894738988\n",
      "Episode : 126/200000, time : 1052, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.294513463337515\n",
      "Episode : 127/200000, time : 1022, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.2907676218896066\n",
      "Episode : 128/200000, time : 1011, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.28711248756127694\n",
      "Updating target_model weights\n",
      "Done updating target model weights\n",
      "Episode : 129/200000, time : 1113, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.283149134021299\n",
      "Episode : 130/200000, time : 1145, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.2791148582495271\n",
      "Dumping : weights_130.hdf5\n",
      "games : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130]\n",
      "avg_reward : [-19, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18]\n",
      "Episode : 131/200000, time : 1066, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.2754271162861363\n",
      "Episode : 132/200000, time : 1128, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.27155717028146886\n",
      "Episode : 133/200000, time : 1120, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.2677817650147705\n",
      "Episode : 134/200000, time : 1128, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.2640324432310479\n",
      "Episode : 135/200000, time : 1118, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.2603616527462278\n",
      "Episode : 136/200000, time : 1095, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.2568189325803067\n",
      "Episode : 137/200000, time : 1111, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.2532737568402293\n",
      "Updating target_model weights\n",
      "Done updating target model weights\n",
      "Episode : 138/200000, time : 1014, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.2500774397951263\n",
      "Episode : 139/200000, time : 1122, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.24660066294439376\n",
      "Episode : 140/200000, time : 1087, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.2432695138082909\n",
      "Dumping : weights_140.hdf5\n",
      "games : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140]\n",
      "avg_reward : [-19, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18]\n",
      "Episode : 141/200000, time : 1106, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.23992337290351792\n",
      "Episode : 142/200000, time : 1219, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.23629220868822506\n",
      "Episode : 143/200000, time : 1048, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.23321689118107963\n",
      "Episode : 144/200000, time : 1012, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.2302736919574889\n",
      "Episode : 145/200000, time : 1118, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.22708359684951734\n",
      "Episode : 146/200000, time : 1186, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.2237362374107697\n",
      "Updating target_model weights\n",
      "Done updating target model weights\n",
      "Episode : 147/200000, time : 1091, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.2207029114848807\n",
      "Episode : 148/200000, time : 1038, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.2178522716751529\n",
      "Episode : 149/200000, time : 1130, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.21479129300411615\n",
      "Episode : 150/200000, time : 1120, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.21180509242613768\n",
      "Dumping : weights_150.hdf5\n",
      "games : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150]\n",
      "avg_reward : [-19, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18]\n",
      "Episode : 151/200000, time : 1020, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.20912165368616423\n",
      "Episode : 152/200000, time : 1129, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.20618334637973515\n",
      "Episode : 153/200000, time : 1126, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.20330665452866623\n",
      "Episode : 154/200000, time : 1019, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.20073088570928838\n",
      "Episode : 155/200000, time : 1118, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.19794016417426005\n",
      "Updating target_model weights\n",
      "Done updating target model weights\n",
      "Episode : 156/200000, time : 1017, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.19543238544874517\n",
      "Episode : 157/200000, time : 1143, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.19265752053578644\n",
      "Episode : 158/200000, time : 1024, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.19020715878493089\n",
      "Episode : 159/200000, time : 1115, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.18757212513613283\n",
      "Episode : 160/200000, time : 1123, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.18495509901185453\n",
      "Dumping : weights_160.hdf5\n",
      "games : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160]\n",
      "avg_reward : [-19, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18]\n",
      "Episode : 161/200000, time : 1117, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.18239282473928564\n",
      "Episode : 162/200000, time : 1122, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.17984806076181736\n",
      "Episode : 163/200000, time : 1021, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.17756061925509195\n",
      "Episode : 164/200000, time : 1015, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.17531980267407496\n",
      "Updating target_model weights\n",
      "Done updating target model weights\n",
      "Episode : 165/200000, time : 1116, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.17289101091724507\n",
      "Episode : 166/200000, time : 1116, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.17049586641137068\n",
      "Episode : 167/200000, time : 1051, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.16826846732957798\n",
      "Episode : 168/200000, time : 1124, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.1659207678802371\n",
      "Episode : 169/200000, time : 1115, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.1636221855913695\n",
      "Episode : 170/200000, time : 1100, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.16137965243496988\n",
      "Dumping : weights_170.hdf5\n",
      "games : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170]\n",
      "avg_reward : [-19, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18]\n",
      "Episode : 171/200000, time : 1017, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.15934303979793288\n",
      "Episode : 172/200000, time : 1113, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.15713558202267564\n",
      "Episode : 173/200000, time : 1115, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.15495870524948627\n",
      "Updating target_model weights\n",
      "Done updating target model weights\n",
      "Episode : 174/200000, time : 1102, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.15284255204249628\n",
      "Episode : 175/200000, time : 1118, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.15071761247504245\n",
      "Episode : 176/200000, time : 1118, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.1486222155192828\n",
      "Episode : 177/200000, time : 1090, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.14660725529328225\n",
      "Episode : 178/200000, time : 1012, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.1447643086538932\n",
      "Episode : 179/200000, time : 1136, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.14272313110109783\n",
      "Episode : 180/200000, time : 1018, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.1409149178296411\n",
      "Dumping : weights_180.hdf5\n",
      "games : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180]\n",
      "avg_reward : [-19, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18]\n",
      "Episode : 181/200000, time : 999, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.13916440111664152\n",
      "Episode : 182/200000, time : 1022, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.1373944046761777\n",
      "Episode : 183/200000, time : 1040, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.13561979301850335\n",
      "Updating target_model weights\n",
      "Done updating target model weights\n",
      "Episode : 184/200000, time : 1019, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.13390157462099256\n",
      "Episode : 185/200000, time : 1014, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.13221173556831411\n",
      "Episode : 186/200000, time : 1024, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.130530168303512\n",
      "Episode : 187/200000, time : 1115, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.12872186945787317\n",
      "Episode : 188/200000, time : 1038, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.127059273876372\n",
      "Episode : 189/200000, time : 1136, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.12526773741187638\n",
      "Episode : 190/200000, time : 1124, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.12351998869335992\n",
      "Dumping : weights_190.hdf5\n",
      "games : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190]\n",
      "avg_reward : [-19, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18]\n",
      "Episode : 191/200000, time : 1110, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.12181489607058985\n",
      "Episode : 192/200000, time : 1029, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.1202595502712332\n",
      "Updating target_model weights\n",
      "Done updating target model weights\n",
      "Episode : 193/200000, time : 1118, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.11858760568956668\n",
      "Episode : 194/200000, time : 1139, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.11690967401737729\n",
      "Episode : 195/200000, time : 1032, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.11541118777705507\n",
      "Episode : 196/200000, time : 1029, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.113931908339175\n",
      "Episode : 197/200000, time : 1108, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.11236478957906187\n",
      "Episode : 198/200000, time : 1130, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.1107859847236785\n",
      "Episode : 199/200000, time : 1017, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.10938786461937489\n",
      "Episode : 200/200000, time : 1025, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.1079911885048401\n",
      "Dumping : weights_200.hdf5\n",
      "games : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200]\n",
      "avg_reward : [-19, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18]\n",
      "Episode : 201/200000, time : 1113, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.10650045829354701\n",
      "Updating target_model weights\n",
      "Done updating target model weights\n",
      "Episode : 202/200000, time : 1029, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.10513539186356814\n",
      "Episode : 203/200000, time : 1015, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.10380858229099539\n",
      "Episode : 204/200000, time : 1124, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.10236023397387403\n",
      "Episode : 205/200000, time : 1045, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.10102802661366143\n",
      "Episode : 206/200000, time : 1008, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.09976302808016826\n",
      "Episode : 207/200000, time : 1133, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.09836128689673551\n",
      "Episode : 208/200000, time : 1129, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.09697924116945668\n",
      "Episode : 209/200000, time : 1023, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.09574578699373702\n",
      "Episode : 210/200000, time : 1056, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.09449021621432353\n",
      "Dumping : weights_210.hdf5\n",
      "games : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210]\n",
      "avg_reward : [-19, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18]\n",
      "Episode : 211/200000, time : 1015, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.09329774885233065\n",
      "Updating target_model weights\n",
      "Done updating target model weights\n",
      "Episode : 212/200000, time : 1023, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.09211111863622014\n",
      "Episode : 213/200000, time : 1119, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.09083051609473602\n",
      "Episode : 214/200000, time : 1017, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.08967974996882053\n",
      "Episode : 215/200000, time : 1019, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.08854356333373568\n",
      "Episode : 216/200000, time : 1011, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.08743051430925856\n",
      "Episode : 217/200000, time : 992, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.08635304312248501\n",
      "Episode : 218/200000, time : 1030, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.085246215582193\n",
      "Episode : 219/200000, time : 1119, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.08406105442094819\n",
      "Episode : 220/200000, time : 1132, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.08287993708547672\n",
      "Dumping : weights_220.hdf5\n",
      "games : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220]\n",
      "avg_reward : [-19, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18]\n",
      "Updating target_model weights\n",
      "Done updating target model weights\n",
      "Episode : 221/200000, time : 1014, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.0818339915484882\n",
      "Episode : 222/200000, time : 1027, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.08078912627252255\n",
      "Episode : 223/200000, time : 1109, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.07967788222122572\n",
      "Episode : 224/200000, time : 1032, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.07865661335558187\n",
      "Episode : 225/200000, time : 1029, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.07764843457299317\n",
      "Episode : 226/200000, time : 1019, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.07666467721831982\n",
      "Episode : 227/200000, time : 1000, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.07571230964195266\n",
      "Episode : 228/200000, time : 1019, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.07475308178553576\n",
      "Episode : 229/200000, time : 1120, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.07371380457410166\n",
      "Episode : 230/200000, time : 1124, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.07268170751089988\n",
      "Dumping : weights_230.hdf5\n",
      "games : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230]\n",
      "avg_reward : [-19, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18]\n",
      "Updating target_model weights\n",
      "Done updating target model weights\n",
      "Episode : 231/200000, time : 1021, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.07176087549790093\n",
      "Episode : 232/200000, time : 1015, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.0708552526162136\n",
      "Episode : 233/200000, time : 1023, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.06995406277231273\n",
      "Episode : 234/200000, time : 1016, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.0690712418831577\n",
      "Episode : 235/200000, time : 1023, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.06819274241568868\n",
      "Episode : 236/200000, time : 1020, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.06732541635835816\n",
      "Episode : 237/200000, time : 1010, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.06648241708563343\n",
      "Episode : 238/200000, time : 1013, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.06564340839566281\n",
      "Episode : 239/200000, time : 1021, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.06481174725335925\n",
      "Updating target_model weights\n",
      "Done updating target model weights\n",
      "Episode : 240/200000, time : 1107, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.06392026999754027\n",
      "Dumping : weights_240.hdf5\n",
      "games : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240]\n",
      "avg_reward : [-19, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -19]\n",
      "Episode : 241/200000, time : 1034, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.06309781880527308\n",
      "Episode : 242/200000, time : 1013, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.062301523775451485\n",
      "Episode : 243/200000, time : 1018, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.06151527799592925\n",
      "Episode : 244/200000, time : 1116, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.060660043334706606\n",
      "Episode : 245/200000, time : 1030, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.059885529325067306\n",
      "Episode : 246/200000, time : 1118, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.05905295273510448\n",
      "Episode : 247/200000, time : 1110, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.058237774923589454\n",
      "Episode : 248/200000, time : 1023, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.05749706355280652\n",
      "Episode : 249/200000, time : 1014, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.05677145010366766\n",
      "Updating target_model weights\n",
      "Done updating target model weights\n",
      "Episode : 250/200000, time : 1004, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.0560634029926369\n",
      "Dumping : weights_250.hdf5\n",
      "games : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250]\n",
      "avg_reward : [-19, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -19, -18]\n",
      "Episode : 251/200000, time : 1030, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.05534481200625183\n",
      "Episode : 252/200000, time : 1023, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.054640895491229295\n",
      "Episode : 253/200000, time : 1089, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.05390279134488098\n",
      "Episode : 254/200000, time : 1022, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.05321721552199832\n",
      "Episode : 255/200000, time : 1107, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.052485219560364546\n",
      "Episode : 256/200000, time : 1027, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.05181508258693378\n",
      "Episode : 257/200000, time : 1015, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.05116117578769718\n",
      "Episode : 258/200000, time : 1027, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.050507944348699534\n",
      "Episode : 259/200000, time : 1016, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.0498705336455579\n",
      "Updating target_model weights\n",
      "Done updating target model weights\n",
      "Episode : 260/200000, time : 1119, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.04917719354636871\n",
      "Dumping : weights_260.hdf5\n",
      "games : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260]\n",
      "avg_reward : [-19, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -19, -18, -18]\n",
      "Episode : 261/200000, time : 1013, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.048556576930089204\n",
      "Episode : 262/200000, time : 1027, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.047936601291678045\n",
      "Episode : 263/200000, time : 1121, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.04727014826892545\n",
      "Episode : 264/200000, time : 1012, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.046675932377485574\n",
      "Episode : 265/200000, time : 1008, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.046089186158525475\n",
      "Episode : 266/200000, time : 1118, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.04545068991501259\n",
      "Episode : 267/200000, time : 1012, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.04487710180742713\n",
      "Episode : 268/200000, time : 1035, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.044299675806434964\n",
      "Updating target_model weights\n",
      "Done updating target model weights\n",
      "Episode : 269/200000, time : 1020, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.04373842647637596\n",
      "Episode : 270/200000, time : 1048, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.04316917559237728\n",
      "Dumping : weights_270.hdf5\n",
      "games : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270]\n",
      "avg_reward : [-19, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -19, -18, -18, -18]\n",
      "Episode : 271/200000, time : 1018, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.04262224899665479\n",
      "Episode : 272/200000, time : 1019, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.042082251620612895\n",
      "Episode : 273/200000, time : 1007, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.04155532866298912\n",
      "Episode : 274/200000, time : 1027, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.041024745721479156\n",
      "Episode : 275/200000, time : 1139, reward:-20.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.0404442742650456\n",
      "Episode : 276/200000, time : 1010, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.03993586396031537\n",
      "Episode : 277/200000, time : 1008, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.03943581648797617\n",
      "Episode : 278/200000, time : 1013, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.038940083140245864\n",
      "Updating target_model weights\n",
      "Done updating target model weights\n",
      "Episode : 279/200000, time : 1017, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.0384467365146317\n",
      "Episode : 280/200000, time : 1015, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.03796153836073128\n",
      "Dumping : weights_280.hdf5\n",
      "games : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, 280]\n",
      "avg_reward : [-19, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -19, -18, -18, -18, -19]\n",
      "Episode : 281/200000, time : 1020, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.03748058928864177\n",
      "Episode : 282/200000, time : 1009, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.03700943439297408\n",
      "Episode : 283/200000, time : 1020, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.03654054788577523\n",
      "Episode : 284/200000, time : 1033, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.03607219050918606\n",
      "Episode : 285/200000, time : 1015, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.03561695914681699\n",
      "Episode : 286/200000, time : 1028, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.03516043984118886\n",
      "Episode : 287/200000, time : 1023, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.034713243195605016\n",
      "Episode : 288/200000, time : 1012, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.03427687560409897\n",
      "Updating target_model weights\n",
      "Done updating target model weights\n",
      "Episode : 289/200000, time : 1015, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.03384430113166603\n",
      "Episode : 290/200000, time : 1016, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.03341718575289042\n",
      "Dumping : weights_290.hdf5\n",
      "games : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, 280, 290]\n",
      "avg_reward : [-19, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -18, -19, -18, -18, -18, -19, -19]\n",
      "Episode : 291/200000, time : 1020, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.03299381080093323\n",
      "Episode : 292/200000, time : 1030, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.03257091362055406\n",
      "Episode : 293/200000, time : 1026, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.03215504466433396\n",
      "Episode : 294/200000, time : 1019, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.031747660254688774\n",
      "Episode : 295/200000, time : 1010, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.03134857193086873\n",
      "Episode : 296/200000, time : 1009, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.03095604820404885\n",
      "Episode : 297/200000, time : 1055, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.03055010334953883\n",
      "Updating target_model weights\n",
      "Done updating target model weights\n",
      "Episode : 298/200000, time : 1000, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.03017059444217185\n",
      "Episode : 299/200000, time : 1012, reward:-21.0, done:True, info:{'ale.lives': 0}, Epsilon : 0.02978984127421833\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "class FrameProcessor(object):\n",
    "    def __init__(self, frame_height=84, frame_width=84):\n",
    "        self.frame_height = frame_height\n",
    "        self.frame_width = frame_width\n",
    "        self.frame = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "        self.processed = tf.image.rgb_to_grayscale(self.frame)\n",
    "        self.processed = tf.image.crop_to_bounding_box(self.processed, 34, 0, 160, 160)\n",
    "        self.processed = tf.image.resize_images(self.processed, \n",
    "                                                [self.frame_height, self.frame_width], \n",
    "                                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    \n",
    "    def __call__(self, session, frame):\n",
    "        return session.run(self.processed, feed_dict={self.frame:frame})\n",
    "\n",
    "    \n",
    "class Atari:\n",
    "    def __init__(self, frame_height=84, frame_width=84, no_of_frames=4):\n",
    "        self.frame_height = frame_height\n",
    "        self.frame_width = frame_width\n",
    "        self.no_of_frames = no_of_frames\n",
    "        self.state = None\n",
    "        self.env = gym.make(\"Pong-v4\")\n",
    "        self.process_frame = FrameProcessor()\n",
    "        \n",
    "    def reset(self):\n",
    "        state = self.env.reset()\n",
    "        state = self.process_frame(session, state)\n",
    "        self.pre_state = np.repeat(state, self.no_of_frames, axis=2) # List of states -- 3d\n",
    "        kk = list((1,))\n",
    "        kk.extend(list(self.pre_state.shape))\n",
    "        self.state = self.pre_state.reshape(tuple(kk)) # One extra dimension to list of states -- 5d\n",
    "        \n",
    "    \n",
    "    def step(self, action):\n",
    "        next_state, reward, done, other_info = self.env.step(action)\n",
    "        next_state = self.process_frame(session, next_state)\n",
    "        self.pre_state = np.append(self.pre_state[:, :, 1:], next_state, axis=2) # -- 4d\n",
    "        kk = list((1,))\n",
    "        kk.extend(list(self.pre_state.shape))\n",
    "        self.state = self.pre_state.reshape(tuple(kk)) #--5d\n",
    "        return self.pre_state[:, :, -1:],reward, done, other_info # returns --4d as 1st\n",
    "\n",
    "\n",
    "    \n",
    "from collections import deque\n",
    "class Agent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=1000000)\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.99995\n",
    "        self.epsilon_min = 0.01\n",
    "        self.learning_rate = 0.00001\n",
    "        self.model = self._create_model()\n",
    "        self.target_model = self._create_model()\n",
    "        \n",
    "    def _create_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Convolution2D(32,(8,8),strides=4, padding=\"valid\", activation='relu',input_shape=self.state_size, kernel_initializer=tf.variance_scaling_initializer(scale=2),use_bias=False))\n",
    "        model.add(Convolution2D(64,(4,4),strides=2, padding=\"valid\", activation='relu', kernel_initializer=tf.variance_scaling_initializer(scale=2),use_bias=False))\n",
    "        model.add(Convolution2D(64,(3,3),strides=1, padding=\"valid\", activation='relu', kernel_initializer=tf.variance_scaling_initializer(scale=2),use_bias=False))\n",
    "        model.add(Convolution2D(64,(7,7),strides=1, padding=\"valid\", activation='relu', kernel_initializer=tf.variance_scaling_initializer(scale=2),use_bias=False))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(self.action_size,activation='softmax'))\n",
    "        model.compile(loss=tf.keras.losses.Huber(),optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            #print(\"Working on random..\")\n",
    "            return random.randrange(self.action_size)\n",
    "        #print(\"Working on prediction..\")\n",
    "        return np.argmax(self.model.predict(state)[0])\n",
    "    \n",
    "    def train(self, batch_size=32):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for experience in minibatch:\n",
    "            state, action, reward, next_state, done = experience\n",
    "            \n",
    "            if not done:\n",
    "                pre_state = state[0]\n",
    "                pre_next_state = np.append(pre_state[:, :, 1:], next_state, axis=2)\n",
    "                kk = list((1,))\n",
    "                kk.extend(list(pre_next_state.shape))\n",
    "                pre_next_state = pre_next_state.reshape(tuple(kk))\n",
    "                main_model_action = np.argmax(self.model.predict(pre_next_state)[0])\n",
    "                target = reward + self.gamma*(self.target_model.predict(pre_next_state)[0][main_model_action])\n",
    "            else:\n",
    "                target = reward\n",
    "            \n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            \n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "            \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    \n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "    \n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "n_episodes = 200000\n",
    "output_dir = \"/content/drive/My Drive/Final_exec_Pong_modelv2/\"\n",
    "batch_size = 32\n",
    "atari = Atari()\n",
    "session = tf.Session()\n",
    "atari.reset()\n",
    "agent = Agent(state_size=atari.pre_state.shape, action_size=atari.env.action_space.n)\n",
    "done = False\n",
    "total_sum = 0\n",
    "games = []\n",
    "avg_reward = []\n",
    "state_number = 0\n",
    "for e in range(n_episodes):\n",
    "    atari.reset()\n",
    "    total_reward = 0\n",
    "    for time in range(18000):\n",
    "        state = (atari.state).copy() # -- 5d\n",
    "        #atari.env.render()\n",
    "        action = agent.act(state)\n",
    "        state_number += 1\n",
    "        next_state, reward, done, other_info = atari.step(action)\n",
    "        total_reward += reward\n",
    "        # next_state -- 4d\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "        if state_number > 50000 and (state_number % 4 == 0):\n",
    "          #print(\"Training the model\")\n",
    "          agent.train(batch_size)\n",
    "        \n",
    "        if state_number > 50000 and (state_number % 10000 == 0):\n",
    "            print(\"Updating target_model weights\")\n",
    "            agent.target_model.set_weights(agent.model.get_weights())\n",
    "            print(\"Done updating target model weights\") \n",
    "        \n",
    "        if done:\n",
    "            print(\"Episode : %s/%s, time : %s, reward:%s, done:%s, info:%s, Epsilon : %s\" % (e, n_episodes, time, total_reward, done,other_info,agent.epsilon))\n",
    "            break\n",
    "    \n",
    "    if e%10 == 0:\n",
    "        print(\"Dumping : %s\" % (\"weights_%s.hdf5\" % e))\n",
    "        agent.save(output_dir + (\"weights_%s.hdf5\" % e))\n",
    "      \n",
    "    total_sum += total_reward\n",
    "    if e!=0 and (e%10 == 0):\n",
    "      games.append(e)\n",
    "      avg_reward.append(int(total_sum/11))\n",
    "      total_sum = 0\n",
    "      print(\"games : %s\" % games)\n",
    "      print(\"avg_reward : %s\" % avg_reward)\n",
    "\n",
    "atari.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "eNzU0h-9KOeR",
    "outputId": "ab607941-fcdc-4152-f097-3ae7bc1c2ea7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xaKwsRCpKPIx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "Final_exec_Pong.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
